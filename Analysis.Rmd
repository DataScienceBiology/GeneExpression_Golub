---
title: "GeneExpressionGolub"
author: "rbabaei"
date: "4/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
library(dplyr)
library(caret)
library(pheatmap)
library(caretEnsemble)
library(doParallel)
library(xgboost)
```

## Introduction

Gloub et al. have published a research in 1999, classifying cancer patients based on gene expression values from microarray analysis. 
Two types of cancer, AML and ALL have been studied. Data is available on kaggle under: https://www.kaggle.com/crawford/gene-expression

```{r data, echo=FALSE}
training <- read.csv("D:/DataScience/Profile/Kaggle_GeneExpression_Golub/data_set_ALL_AML_train.csv", stringsAsFactors = FALSE, quote = "")
testing <- read.csv("D:/DataScience/Profile/Kaggle_GeneExpression_Golub/data_set_ALL_AML_independent.csv", stringsAsFactors = FALSE, quote = "")
colData <- read.csv("D:/DataScience/Profile/Kaggle_GeneExpression_Golub/actual.csv", stringsAsFactors = FALSE, quote = "")
colData$patient <- paste("X", colData$patient, sep = "")
colData_train <- colData[1:38,]
colData_test <- colData[39:72,]
```

## Data Processing

First we need to clean the data, remove unwanted variables (Call, and Gene Description), order the observation, transpose to have genes as attribute, and bind the cancer type
to the table.

```{r processing}
trainset <- training %>% select(-contains("call"), -Gene.Description)
trainset <- trainset[,c(1:28,34:39,29:33)]
rownames(trainset) <- trainset$Gene.Accession.Number
trainset <- trainset[,-1]

testset <- testing %>% select(-contains("call"), - Gene.Description)
testset <- testset[,order(names(testset))]
rownames(testset) <- testset$Gene.Accession.Number
testset <- testset[,-1]

```

Next, we check for missing values, if any. 

```{r missVal}
sum(is.na(trainset))
sum(is.na(testset))
```

There is no missing value. Let's see how the data looks like.

```{r boxPlot}
boxplot(trainset, outline = F, col = "green")
```

There was some normalization done. Expression values per sample looks to have same scale. Long tails are due to outliers. Now ew look at the distribution of one of the genes through whole samples.

```{r hist1}
hist(trainset[,5], xlab = "gene expression", border = "blue4", col = "green")
```

As we see the distribution looks skewed. Now, we transpose the data and try some normalization. We'll also select the 100 most variable genes through samples. 

```{r centered}
trainset <- as.data.frame(t(trainset))
testset <- as.data.frame(t(testset))

# select top 100 variable genes
SDs = apply(trainset, 2, sd)
topPreds = order(SDs, decreasing = TRUE)[1:100]
trainset = trainset[,topPreds]

# centering and scaling
Centered <- preProcess(trainset, method = c("center"))
trainset <- predict(Centered, trainset)

hist(trainset[,5], xlab = "gene expression", border = "blue4", col = "green")
```

After scaling it looks promissing. We add the cancer types to dataframe before fit our first model.

```{r colData}
trainset <- cbind(trainset, colData_train$cancer)
colnames(trainset)[101] <- "Cancer"
```

## Prediction Models

Here we'll ensemble three models, to compare their individual performance and the ensembled model.

```{r modelList}
grid.xgb <- expand.grid(.nrounds = 200, .eta = c(0.01, 0.001, 0.0001),
                        .max_depth = c(2,4), .gamma = 0, .colsample_bytree = 0.05, .subsample = 0.05,
                        .min_child_weight = 1)

grid.rf <- expand.grid(.mtry=50,
                       .min.node.size = 5,
                       .splitrule="gini")
grid.enet <- expand.grid(.alpha = 0.5, .lambda=seq(0.1,0.7,0.05))

registerDoParallel(cores = 3)

set.seed(428)
my_control <- trainControl(method = "repeatedcv", number = 5, repeats = 3,
                           savePredictions = "final",classProbs = TRUE, allowParallel = TRUE)

set.seed(430)
model_list <- caretList(Cancer~., data = trainset,
                        trControl = my_control,
                        tuneList = list(
                          xgb = caretModelSpec(method = "xgbTree", tuneGrid = grid.xgb),
                          rf = caretModelSpec(method = "ranger", tuneGrid = grid.rf),
                          enet = caretModelSpec(method = "glmnet", tuneGrid = grid.enet)
                        ))


stopImplicitCluster()

data.frame(xgb =  model_list$xgb$results[1,8],
           rf = model_list$rf$results$Accuracy,
           enet = model_list$enet$results[2,3],
           row.names = "Accuracy")

resample <- resamples(model_list)
modelCor(resample) # models are not highly correlated, thus we can go for ensembling 

dotplot(resample, metric = "Accuracy")

```

It seems that enet provides the best Accuracy. Next, we perform a linear combination of all models.

```{r ensemble1}
registerDoSEQ()
ensemble_1 <- caretEnsemble(model_list,
                            metric = "Accuracy",
                            trControl = my_control)

summary(ensemble_1)# the accuracy of ensemble: 0.94

# plot the ensamble
plot(ensemble_1)
```

The combination provides better accuracy than each individual models. We can also fit an ensemble model through a regularized model combination.

```{r ensemble2}
registerDoSEQ()
ensemble_2 <- caretStack(model_list,
                         method = "glmnet",
                         metric = "Accuracy",
                         trControl = my_control)

print(ensemble_2)#  accuracy = 0.94

plot(varImp(ensemble_2$models$enet), top = 10)
```

The secound model is doing as well as the first one. The 10 most important genes in predicting the cancer types are depicted in the graph above. 
Finally, we should verify the performance of our models on testing dataset. Before that, the testing data should be handled as training.

```{r testing}
col_names <- names(trainset)
col_ID <- which(names(testset) %in% col_names)

testset <- testset[,col_ID]
Centered <- preProcess(testset, method = c("center"))
testset <- predict(Centered, testset)
```

The cancer types will be predicted using the two ensamble models.

```{r prediction}
pred_ensemble1 <- predict(ensemble_1, newdata =  testset)
pred_ensemble2 <- predict(ensemble_2, newdata = testset)

pred_accuracy <- data.frame(ensemble_1 = confusionMatrix(as.factor(colData_test$cancer), pred_ensemble1)$overall[1],
                            ensemble_2 = confusionMatrix(as.factor(colData_test$cancer), pred_ensemble2)$overall[1])
pred_accuracy
```

Although both models were doing the same type during training, ensemble2 is more accurate than ensemble1 in predicting the cancer type on new dataset.

## Conclusion

This dataset has been already processed, which makes it somehow dificult to normalize it in a better way. We don't have any information about the way it has been processed. 
The info could be perhaps available in the original publication. In addition,  the low number of samples doesn't let to find a better solution. Although we got an accuracy of 0.94, our fitted models need to be qualified using more data, to be able to improve the models with extra tunings.

